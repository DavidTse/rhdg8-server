= Red Hat Data Grid 8 server
Álvaro López Medina <alopezme@redhat.com>
v1.0, 2020-11
// Create TOC wherever needed
:toc: macro
:sectanchors:
:sectnumlevels: 2
:sectnums: 
:source-highlighter: pygments
:imagesdir: images
// Start: Enable admonition icons
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
ifndef::env-github[]
:icons: font
endif::[]
// End: Enable admonition icons

This repository demonstrates some of the basic features of the latest release of Red Hat Data Grid 8 and how to deploy a RHDG cluster on OCP and RHEL. 

// Create the Table of contents here
toc::[]

== Introduction

Red Hat Data Grid is an in-memory, distributed, NoSQL datastore solution. Your applications can access, process, and analyze data at in-memory speed to deliver a superior user experience. 

Red Hat Data Grid provides value as a standard architectural component in application infrastructures for a variety of real-world scenarios and use cases:

* Data caching and transient data storage.
* Primary data store.
* Low latency compute grid.


=== Features and benefits

To support modern data management requirements with rapid data processing, elastic scalability, and high availability, Red Hat Data Grid offers: 

* *NoSQL data store*. Provides simple, flexible storage for a variety of data without the constraints of a fixed data model.
* *Apache Spark and Hadoop integration*. Offers full support as an in-memory data store for Apache Spark and Hadoop, with support for Spark resilient distributed datasets (RDDs) and Discretized Streams (Dstreams), as well as Hadoop I/O format.
* *Rich querying*. Provides easy search for objects using values and ranges, without the need for key-based lookups or an object’s exact location. 
* *Polyglot client and access protocol support*. Offers read/write capabilities that let applications written in multiple programming languages easily access and share data. Applications can access the data grid remotely, using REST, or Hot Rod—for Java™, C++, and .NET.
* *Distributed parallel execution*. Quickly process large volumes of data and support long-running compute applications using simplified Map-Reduce parallel operations.

* *Flexible persistence*. Increase the lifespan of information in the memory for improved durability through support for both shared nothing and shared database—RDBMS or NoSQL—architectures.

* *Comprehensive security*. Authentication, role-based authorization, and access control are integrated with existing security and identity structures to give only trusted users, services, and applications access to the data grid.

* *Cross-datacenter replication*. Replicate applications across datacenters and achieve high availability to meet service-level agreement (SLA) requirements for data within and across datacenters.

* *Rolling upgrades*. Upgrade your cluster without downtime for continuous, uninterrupted operations for remote users and applications.


=== RHDG Operator

The Data Grid Operator provides operational intelligence and reduces management complexity for deploying Data Grid on OpenShift. Data Grid Operator automatically upgrades Data Grid clusters when new versions become available.

To upgrade Data Grid clusters, Data Grid Operator checks the version of the image for Data Grid nodes. If Data Grid Operator determines that a new version of the image is available, it gracefully shuts down all nodes, applies the new image, and then restarts the nodes.
On Red Hat OpenShift, the Operator Lifecycle Manager (OLM) enables upgrades for Data Grid Operator. 






== Deploying RHDG on RHEL

This section explains how to configure, run, and monitor Data Grid servers. RHDG server is available on the RH https://access.redhat.com/jbossnetwork/restricted/listSoftware.html?product=data.grid&downloadType=distributions[downloads website]. 



=== Running RHDG locally in standalone mode

This is the basic installation option, valid for basic exploration and development purposes. Follow this steps to run the server in this mode:

.Unzip the server
[source, bash]
----
cd ~/Downloads
unzip redhat-datagrid-8.1.0-server.zip
----

.Create the developer user
[source, bash]
----
./redhat-datagrid-8.1.0-server/bin/cli.sh user create developer -p developer
----

.Launch the server
[source, bash]
----
./redhat-datagrid-8.1.0-server/bin/server.sh 
16:33:08,457 INFO  (main) [BOOT] JVM OpenJDK 64-Bit Server VM Red Hat, Inc. 11.0.9+11
[...]
16:33:13,327 INFO  (main) [org.infinispan.SERVER] ISPN080004: Protocol SINGLE_PORT listening on 127.0.0.1:11222
16:33:13,327 INFO  (main) [org.infinispan.SERVER] ISPN080034: Server 'hat-42208' listening on http://127.0.0.1:11222
16:33:13,327 INFO  (main) [org.infinispan.SERVER] ISPN080001: Red Hat Data Grid Server 8.1.0.GA started in 4806ms
----


=== Running RHDG locally in clustering mode

Unzip the RHDG server as in the previous example and duplicate the server folder to use one with each deployment. 

.Unzip the server
[source, bash]
----
cd ~/Downloads
unzip redhat-datagrid-8.1.0-server.zip
----

.Duplicate the server config
[source, bash]
----
cp redhat-datagrid-8.1.0-server/server redhat-datagrid-8.1.0-server/server-01
cp redhat-datagrid-8.1.0-server/server redhat-datagrid-8.1.0-server/server-02
----


.Create the developer user in both instances
[source, bash]
----
./redhat-datagrid-8.1.0-server/bin/cli.sh user create developer -p developer --server-root=server-01
./redhat-datagrid-8.1.0-server/bin/cli.sh user create developer -p developer --server-root=server-02
----


.Launch both server instances
[source, bash]
----
./redhat-datagrid-8.1.0-server/bin/server.sh --node-name=node-01 --server-root=redhat-datagrid-8.1.0-server/server-01 --port-offset=0
./redhat-datagrid-8.1.0-server/bin/server.sh --node-name=node-02 --server-root=redhat-datagrid-8.1.0-server/server-02 --port-offset=100
----

After running both commands, you will similar logs to these in both terminals:
[source, bash]
----
[...]
20:19:29,614 INFO  (main) [org.infinispan.SERVER] ISPN080034: Server 'node-01' listening on http://127.0.0.1:11222
20:19:29,614 INFO  (main) [org.infinispan.SERVER] ISPN080001: Red Hat Data Grid Server 8.1.0.GA started in 5585ms
20:19:36,637 INFO  (jgroups-8,node-01) [org.infinispan.CLUSTER] ISPN000094: Received new cluster view for channel cluster: [node-01|1] (2) [node-01, node-02]
20:19:36,647 INFO  (jgroups-8,node-01) [org.infinispan.CLUSTER] ISPN100000: Node node-02 joined the cluster
20:19:37,365 INFO  (jgroups-5,node-01) [org.infinispan.CLUSTER] [Context=org.infinispan.CLIENT_SERVER_TX_TABLE]ISPN100002: Starting rebalance with members [node-01, node-02], phase READ_OLD_WRITE_ALL, topology id 2
[...]
20:19:38,463 INFO  (jgroups-5,node-01) [org.infinispan.CLUSTER] [Context=___hotRodTopologyCache_hotrod]ISPN100010: Finished rebalance with members [node-01, node-02], topology id 5
----


== Deploying RHDG on OCP using the Operator

An Operator is a method of packaging, deploying and managing a Kubernetes-native application. A Kubernetes-native application is an application that is both deployed on Kubernetes and managed using the Kubernetes APIs and kubectl tooling.

Install Data Grid Operator into a OpenShift namespace to create and manage Data Grid clusters.

=== Deploying the RHDG operator

Create subscriptions to Data Grid Operator on OpenShift so you can install different Data Grid versions and receive automatic updates.

To deploy the RHDG operator, you will need to create three different objects:

* Two *Openshift projects* that will contain the operator and the objects of the RHDG cluster.

* An *OperatorGroup*, which configures all Operators deployed in the same namespace as the OperatorGroup object to watch for their custom resource (CR) in a list of namespaces or cluster-wide. Basically, you will need one in your namespace because you are not creating the operator one of the default Openshift projects, such as `openshift-operators`.

* A *Subscription*, which represents an intention to install an Operator. It is the custom resource that relates an Operator to a CatalogSource. Subscriptions describe which channel of an Operator package to subscribe to, and whether to perform updates automatically or manually. 

I have created an OCP template to quickly deploy this operator. Just execute the following command have it up and running on your cluster. 

IMPORTANT: Bear in mind that you will need `cluster-admin` permissions to deploy an operator, as it is necessary to create cluster-wide CRDs (Custom Resource Definitions).

[source, bash]
----
oc process -f templates/rhdg-01-operator.yaml | oc apply -f -
----

This template provides two parameters to modify the project where the operator and the cluster is installed. It is possible to deploy both on the same project or in different projects. By default, values are: 

* *OPERATOR_NAMESPACE* = `rhdg8-operator`
* *CLUSTER_NAMESPACE* = `rhdg8`

Modify them just passing arguments to the template:

[source, bash]
----
oc process -f templates/rhdg-01-operator.yaml -p OPERATOR_NAMESPACE="other-namespace" -p CLUSTER_NAMESPACE="another-namespace" | oc apply -f -
----

It is also possible to install the operator from the web console. For more information, please check the official https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.1/html-single/running_data_grid_on_openshift/index#installation[documentation].


=== Deploying a RHDG cluster

Data Grid Operator lets you create, configure, and manage Data Grid clusters. Data Grid Operator adds a new Custom Resource (CR) of type Infinispan that lets you handle Data Grid clusters as complex units on OpenShift.

Data Grid Operator watches for Infinispan Custom Resources (CR) that you use to instantiate and configure Data Grid clusters and manage OpenShift resources, such as StatefulSets and Services. In this way, the Infinispan CR is your primary interface to Data Grid on OpenShift.


I have created an OCP template to quickly deploy a basic RHDG cluster with 3 replicas. Execute the following command have it up and running on your cluster. 


[source, bash]
----
oc process -f templates/rhdg-02-cluster.yaml | oc apply -f -
----

This template provides two parameters to modify the project where the cluster is installed and the name of the cluster to deploy. The cluster namespace should be the same as in the previous step. By default, values are: 

* *CLUSTER_NAMESPACE* = `rhdg8`
* *CLUSTER_NAME* = `rhdg`


Modify them just passing arguments to the template:

[source, bash]
----
oc process -f templates/rhdg-02-cluster.yaml -p CLUSTER_NAMESPACE="another-namespace" -p CLUSTER_NAME="my-cluster" | oc apply -f -
----


=== Monitoring RHDG with Prometheus

Data Grid exposes a metrics endpoint that provides statistics and events to Prometheus.

After installing OpenShift Container Platform 4.6, cluster administrators can optionally enable monitoring for user-defined projects. By using this feature, cluster administrators, developers, and other users can specify how services and pods are monitored in their own projects. You can then query metrics, review dashboards, and manage alerting rules and silences for your own projects in the OpenShift Container Platform web console. We are going to take advantage of this feature.


.Enabling monitoring for user-defined projects
[WARNING]
==== 
Monitoring of user-defined projects is not enabled by default. To enable it, you need to modify a Configmap of the `openshift-monitoring`. You need permissions to create and modify Configmaps in this project.

[source, bash]
----
oc apply -f templates/rhdg-02-ocp-user-workload-monitoring.yaml
----
====



I have created an OCP template to quickly configure metrics monitorization of a RHDG cluster. Execute the following command:

[source, bash]
----
oc process -f templates/rhdg-03-monitoring.yaml | oc apply -f -
----

This template provides two parameters to modify the project where the cluster was installed and the name of the cluster itself. By default, values are: 

* *CLUSTER_NAMESPACE* = `rhdg8`
* *CLUSTER_NAME* = `rhdg`


Modify them just passing arguments to the template:

[source, bash]
----
oc process -f templates/rhdg-03-monitoring.yaml -p CLUSTER_NAMESPACE="another-namespace" -p CLUSTER_NAME="my-cluster" | oc apply -f -
----

For more information, access the Openshift https://docs.openshift.com/container-platform/4.6/monitoring/understanding-the-monitoring-stack.html[documentation] for the monitoring stack and the RHDG documenation to https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.1/html-single/running_data_grid_on_openshift/index#prometheus[configure monitoring] for RHDG 8 on OCP.



=== Monitoring RHDG with Grafana

A typical OpenShift monitoring stack includes Prometheus for monitoring both systems and services, and Grafana for analyzing and visualizing metrics.

Administrators are often looking to write custom queries and create custom dashboards in Grafana. However, Grafana instances provided with the monitoring stack (and its dashboards) are read-only.  To solve this problem, we can use the community-powered Grafana operator provided by OperatorHub.

To deploy the community-powered Grafana operator on OCP 4.6 just follow these steps:

==== Deploy the Grafana operator
[source, bash]
----
oc process -f templates/grafana-01-operator.yaml | oc apply -f -
----

==== Create a Grafana instance
Now, we will create a Grafana instance using the operator:
[source, bash]
----
oc process -f templates/grafana-02-instance.yaml | oc apply -f -
----

==== Create a Grafana data source
Now, we will create a Grafana data source:
[source, bash]
----
oc adm policy add-cluster-role-to-user cluster-monitoring-view -z grafana-serviceaccount

PROJECT=grafana-operator
BEARER_TOKEN=$(oc serviceaccounts get-token grafana-serviceaccount -n ${PROJECT})

oc process -f templates/grafana-03-datasource.yaml -p BEARER_TOKEN=$BEARER_TOKEN| oc apply -f -
----

==== Create a Grafana dashboard
Now, we will create a Grafana dashboard:
[source, bash]
----
#TODO: Please complete
----


For more information, access the Grafana https://grafana.com/docs/grafana/latest/[main documentation] or the Grafana https://github.com/integr8ly/grafana-operator/blob/v3.6.0/README.md[operator documentation].



=== Configure cluster's caches

Although there are many options to configure RHDG caches, here we will explore the two more useful for this configuration: Exposing the REST API and using the Operator's CRD.

==== Using REST API

First define a cache using the JSON format. The following configuration is an example of cache configuration. Bear in mind that the cache configuration should not have the cache name, as the name will be inherited from the REST URL: 

[source, json]
----
{
    "distributed-cache": {
        "mode": "SYNC",
        "owners": 1,
        "partition-handling": {
            "when-split": "ALLOW_READ_WRITES",
            "merge-policy": "REMOVE_ALL"
        },
        "transaction": {
            "mode": "NONE"
        },
        "memory": {
            "off-heap": {
            "size": 96468992,
            "eviction": "MEMORY",
            "strategy": "REMOVE"
            }
        },
        "statistics": true
    }
}
----

===== Configuration
[source, bash]
----
CLUSTER_NAMESPACE="rhdg8"
CLUSTER_NAME="rhdg"
RHDG_URL=$(oc get route ${CLUSTER_NAME}-external -n ${CLUSTER_NAMESPACE} -o template='https://{{.spec.host}}')
CACHE_NAME="distributed-rest-01"
----

===== Cache creation
Create your cache using the following variables and the curl command:
[source, bash]
----

curl -X POST -k -u developer:developer -H "Content-Type: application/json" ${RHDG_URL}/rest/v2/caches/${CACHE_NAME} --data-binary "@caches/distributed-01.json"
----

Check if the cache was created successfully using the following command:
[source,bash]
----
curl -X GET -k -u developer:developer -H "Content-Type: application/json" ${RHDG_URL}/rest/v2/caches
----

===== Cache usage
Perform gets and puts to check that the cache is working properly:
[source,bash]
----
curl -X POST -k -u developer:developer ${RHDG_URL}/rest/v2/caches/${CACHE_NAME}/0 --data "Hello World"
curl -X GET -k -u developer:developer  ${RHDG_URL}/rest/v2/caches/${CACHE_NAME}/0
----


For more information about the REST endpoint, check the https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.1/html-single/data_grid_rest_api/index#rest_v2_create_cache[documentation].



==== Using Cache operator's CRD


WARNING: This feature is Tech Preview as of December, 2020. Use it for development purposes only.








== Annex: Deploying the Infinispan operator

The same configuration rules from the previous chapter apply.

[source, bash, linenums]
----
oc process -f templates/infinispan-01-operator.yaml -p OPERATOR_NAMESPACE="infinispan-operator" -p CLUSTER_NAMESPACE="infinispan" | oc apply -f -

oc process -f templates/infinispan-02-cluster.yaml -p CLUSTER_NAMESPACE="infinispan" -p CLUSTER_NAME="infinispan" | oc apply -f -

----

It is also possible to install the operator from the web console. For more information, please check the official https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.1/html-single/running_data_grid_on_openshift/index#installation[documentation].




== Annex: Managing SSL certificates from the client side

There are two options to manage SSL configuration for the Java HotRod client.

=== Option 1: Default configuration using operator

First option is to use the default configuration. This is the recommended configuration.

* The RHDG operator provides certificates by default in a secret with name `${RHDG_CLUSTER_NAME}-cert-secret`.
* The Infinispan Spring Starter accepts a certificate in `.pem` format and builds an in-memory KeyStore with all the certificates found under a path provided.
* The `infinispan-client-hotrod` library provides similar functionality.

Add the following lines to your `application.properties` to configure Infinispan Spring Starter:
[source, bash]
----
infinispan.remote.use-ssl=true
infinispan.remote.trust-store-path=config/tls.crt
infinispan.remote.sni-host-name=${RHDG_CLUSTER_NAME}.${CLUSTER_NAMESPACE}.svc
----

Add the following lines to your `application.properties` to configure infinispan-client-hotrod:
[source, bash]
----
infinispan.client.hotrod.use_ssl=true
infinispan.client.hotrod.trust_store_path=config/tls.crt
infinispan.client.hotrod.sni_host_name=${RHDG_CLUSTER_NAME}.${CLUSTER_NAMESPACE}.svc
----

For more information about configuration parameters check the following resources:

* https://access.redhat.com/webassets/avalon/d/red-hat-data-grid/8.1/api/org/infinispan/client/hotrod/configuration/package-summary.html[RHDG 8.1 JavaDoc].
* https://github.com/infinispan/infinispan-spring-boot/blob/master/infinispan-spring-boot-starter-remote/src/test/resources/test-application.properties[Testing configuration of the Spring Starter].


=== Option 2: Custom truststore using openssl

The second option is useful when you want custom certificates or you would like to access RHDG from outside your cluster.

The following commands are inspired in the https://github.com/infinispan/infinispan-image-artifacts/blob/9f028ddc1f5e26084b0b0edf46feb11ff3df2570/config-generator/src/main/groovy/org/infinispan/images/ConfigGenerator.groovy#L74-L77[code executed by the RHDG operator] to build SSL configuration from the server side.


[source, bash]
----
# Some variables
export CLUSTER_NAME=rhdg
export PCKS12_PASSWORD=changeit
export PCKS12_ALIAS=rhdg

# Get both tls elements in .pem format 
oc get secret ${CLUSTER_NAME}-cert-secret -o jsonpath='{.data.tls\.key}' | base64 --decode > server.pem
oc get secret ${CLUSTER_NAME}-cert-secret -o jsonpath='{.data.tls\.crt}' | base64 --decode >> server.pem

# Create the PKCS file
openssl pkcs12 -export -passout env:PCKS12_PASSWORD -inkey tls.key -in tls.crt -name $PCKS12_ALIAS -out keystore.pkcs12
# Check that it worked
openssl pkcs12 -nokeys -info -in keystore.pkcs12 -passin pass:${PCKS12_PASSWORD}

# Convert to JKS
keytool -importkeystore -srckeystore keystore.pkcs12 -srcstoretype PKCS12 -destkeystore truststore.jks -deststoretype JKS -srcstorepass ${PCKS12_PASSWORD} -storepass ${PCKS12_PASSWORD}
# Create secret with the .jks file
oc create secret generic rhdg-client-truststore-secret --from-file=truststore.jks
----

After that, you would need to mount the `.jks` into the container and configure the JKS truststore (Similar as done in the previous option).

=== Option 3: initContainers

There is another option that I will document, but it is explained https://developers.redhat.com/blog/2017/11/22/dynamically-creating-java-keystores-openshift/[here]. This is an elegant solution for applications that use SSL but do not accept certificates in `.pem` format. The downside is that you are forced to use some kind of persistence.







== Useful links

* https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.1/[RHDG 8.1 documentation].
* https://infinispan.org/documentation[Upstream documentation].
* https://access.redhat.com/articles/4933551[RHDG 8 Supported Configurations].
* https://access.redhat.com/articles/4933371[RHDG 8 Component Details].
* https://access.redhat.com/articles/4961121[RHDG 8 Maintenance Schedule].
* https://access.redhat.com/support/policy/updates/jboss_notes/#p_rhdg[RHDG Product Update and Support Policy].
* https://developers.redhat.com/blog/2020/10/15/securely-connect-quarkus-and-red-hat-data-grid-on-red-hat-openshift[Securely connect Quarkus and RHDG 8.1 on OCP].