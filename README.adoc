= Red Hat Data Grid 8 server
Álvaro López Medina <alopezme@redhat.com>
v1.0, 2020-11
// Create TOC wherever needed
:toc: macro
:sectanchors:
:sectnumlevels: 2
:sectnums: 
:source-highlighter: pygments
:imagesdir: images
// Start: Enable admonition icons
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
ifndef::env-github[]
:icons: font
endif::[]
// End: Enable admonition icons

This repository demonstrates some of the basic features of Red Hat's latest Data Grid release and how to deploy a Data Grid cluster on OCP and RHEL. 

// Create the Table of contents here
toc::[]

== Introduction

Red Hat Data Grid is an in-memory, distributed, NoSQL datastore solution. Your applications can access, process, and analyze data at in-memory speed to deliver a superior user experience. 

Red Hat Data Grid provides value as a standard architectural component in application infrastructures for a variety of real-world scenarios and use cases:

* Data caching and transient data storage.
* Primary data store.
* Low latency compute grid.


=== Features and benefits

To support modern data management requirements with rapid data processing, elastic scalability, and high availability, Red Hat Data Grid offers: 

* *NoSQL data store*. Provides simple, flexible storage for a variety of data without the constraints of a fixed data model.
* *Apache Spark and Hadoop integration*. Offers full support as an in-memory data store for Apache Spark and Hadoop, with support for Spark resilient distributed datasets (RDDs) and Discretized Streams (Dstreams), as well as Hadoop I/O format.
* *Rich querying*. Provides easy search for objects using values and ranges, without the need for key-based lookups or an object’s exact location. 
* *Polyglot client and access protocol support*. Offers read/write capabilities that let applications written in multiple programming languages easily access and share data. Applications can access the data grid remotely, using REST, or Hot Rod—for Java™, C++, and .NET.
* *Distributed parallel execution*. Quickly process large volumes of data and support long-running compute applications using simplified Map-Reduce parallel operations.

* *Flexible persistence*. Increase the lifespan of information in the memory for improved durability through support for both shared nothing and shared database—RDBMS or NoSQL—architectures.

* *Comprehensive security*. Authentication, role-based authorization, and access control are integrated with existing security and identity structures to give only trusted users, services, and applications access to the data grid.

* *Cross-datacenter replication*. Replicate applications across datacenters and achieve high availability to meet service-level agreement (SLA) requirements for data within and across datacenters.

* *Rolling upgrades*. Upgrade your cluster without downtime for continuous, uninterrupted operations for remote users and applications.


=== RHDG Operator

The Data Grid Operator provides operational intelligence and reduces management complexity for deploying Data Grid on OpenShift, automatically upgrading clusters when new image versions become available.

In order to upgrade Data Grid clusters, the operator checks the version of the image of each Data Grid node. If the operator determines that a new version of the image is available, it gracefully shuts down all nodes, applies the new image, and restarts the nodes.

On OpenShift, the Operator Lifecycle Manager (OLM) enables upgrades for Data Grid Operator. 






== Deploying RHDG on RHEL

This section explains how to configure, run, and monitor Data Grid servers. The binaries are available on the RH https://access.redhat.com/jbossnetwork/restricted/listSoftware.html?product=data.grid&downloadType=distributions[downloads website]. 



=== Running RHDG locally in standalone mode

This is the basic installation option, valid for basic exploration and development purposes. Follow these steps to run the server in this mode:

.Unzip the server
[source, bash]
----
cd ~/Downloads
unzip redhat-datagrid-8.2.0-server.zip
----

.Create the developer user
[source, bash]
----
./redhat-datagrid-8.2.0-server/bin/cli.sh user create developer -p developer
----

.Launch the server
[source, bash]
----
./redhat-datagrid-8.2.0-server/bin/server.sh 
2021-06-22 15:27:20,379 INFO  (main) [BOOT] JVM OpenJDK 64-Bit Server VM Red Hat, Inc. 11.0.11+9
[...]
2021-06-22 15:27:30,674 INFO  (main) [org.infinispan.SERVER] ISPN080004: Connector SINGLE_PORT (default) listening on 127.0.0.1:11222
2021-06-22 15:27:30,674 INFO  (main) [org.infinispan.SERVER] ISPN080034: Server 'hat-59097' listening on http://127.0.0.1:11222
2021-06-22 15:27:30,773 INFO  (main) [org.infinispan.SERVER] ISPN080001: Red Hat Data Grid Server 8.2.0.GA started in 10278ms
----


=== Running RHDG locally in clustering mode

Unzip the server as in the previous example, locate the folder named `server`, duplicate it and then use one with each server instance. 

.Unzip the server
[source, bash]
----
cd ~/Downloads
unzip redhat-datagrid-8.2.0-server.zip
----

.Duplicate the server config
[source, bash]
----
cp -r redhat-datagrid-8.2.0-server/server redhat-datagrid-8.2.0-server/server-01
cp -r redhat-datagrid-8.2.0-server/server redhat-datagrid-8.2.0-server/server-02
----


.Create the developer user in both instances
[source, bash]
----
./redhat-datagrid-8.2.0-server/bin/cli.sh user create developer -p developer --server-root=server-01
./redhat-datagrid-8.2.0-server/bin/cli.sh user create developer -p developer --server-root=server-02
----


.Launch both server instances
[source, bash]
----
./redhat-datagrid-8.2.0-server/bin/server.sh --node-name=node-01 --server-root=redhat-datagrid-8.2.0-server/server-01 --port-offset=0
./redhat-datagrid-8.2.0-server/bin/server.sh --node-name=node-02 --server-root=redhat-datagrid-8.2.0-server/server-02 --port-offset=100
----

After running both commands, you will see in both terminals similar logs to the ones shown below:
[source, bash]
----
[...]
2021-06-22 15:29:23,421 INFO  (main) [org.infinispan.SERVER] ISPN080034: Server 'node-01' listening on http://127.0.0.1:11222
2021-06-22 15:29:23,549 INFO  (main) [org.infinispan.SERVER] ISPN080001: Red Hat Data Grid Server 8.2.0.GA started in 19445ms
2021-06-22 15:29:25,058 INFO  (jgroups-6,node-01) [org.infinispan.CLUSTER] ISPN000094: Received new cluster view for channel cluster: [node-01|1] (2) [node-01, node-02]
2021-06-22 15:29:25,112 INFO  (jgroups-6,node-01) [org.infinispan.CLUSTER] ISPN100000: Node node-02 joined the cluster
2021-06-22 15:29:27,100 INFO  (jgroups-6,node-01) [org.infinispan.CLUSTER] [Context=org.infinispan.CLIENT_SERVER_TX_TABLE]ISPN100002: Starting rebalance with members [node-01, node-02], phase READ_OLD_WRITE_ALL, topology id 2
[...]
2021-06-22 15:29:29,175 INFO  (jgroups-12,node-01) [org.infinispan.CLUSTER] [Context=___hotRodTopologyCache_hotrod-default]ISPN100010: Finished rebalance with members [node-01, node-02], topology id 5
----


== Deploying RHDG on OCP using the Operator

An Operator is a method of packaging, deploying and managing a Kubernetes-native application. A Kubernetes-native application is an application that is both deployed on Kubernetes and managed using the Kubernetes APIs and kubectl tooling.

Install Data Grid Operator into a OpenShift namespace to create and manage Data Grid clusters.

=== Deploying the RHDG operator

Create subscriptions to Data Grid Operator on OpenShift so you can install different Data Grid versions and receive automatic updates.

To deploy the RHDG operator, you will need to create three different objects:

* Two *Openshift projects* that will contain the operator and the objects of the RHDG cluster.

* An *OperatorGroup*, which provides multitenant configuration to OLM-installed Operators. An Operator group selects target namespaces in which to generate required RBAC access for its member Operators. As we are not deploying our operator in the default namespace (`openshift-operators`), we will need to create one to set the namespaces where the Data Grid operator will be able to create and monitorize clusters.

NOTE: The *OperatorGroup* resource allows to configure https://docs.openshift.com/container-platform/4.6/operators/understanding/olm/olm-understanding-operatorgroups.html#olm-operatorgroups-membership_olm-understanding-operatorgroups[four possible namespace-scopes] for the operator. Please check <<annex-configure-the-scope-of-your-operator>> before executing the commands of this section.

* A *Subscription*, which represents an intention to install an Operator. It is the custom resource that relates an Operator to a CatalogSource. Subscriptions describe which channel of an Operator package to subscribe to, and whether to perform updates automatically or manually. 

I have created an OCP template to quickly deploy this operator. Just execute the following command have it up and running on your cluster. 

IMPORTANT: Bear in mind that you will need `cluster-admin` permissions to deploy an operator, as it is necessary to create cluster-wide CRDs (Custom Resource Definitions).

[source, bash]
----
oc process -f rhdg-operator/rhdg-01-operator.yaml | oc apply -f -
----

This template provides two parameters to modify the project where the operator and the cluster is installed. It is possible to deploy both on the same project or in different projects. By default, values are: 

* *OPERATOR_NAMESPACE* = `rhdg8-operator`
* *CLUSTER_NAMESPACE* = `rhdg8`

Modify them just passing arguments to the template:

[source, bash]
----
oc process -f rhdg-operator/rhdg-01-operator.yaml -p OPERATOR_NAMESPACE="other-namespace" -p CLUSTER_NAMESPACE="another-namespace" | oc apply -f -
----

It is also possible to install the operator from the web console. For more information, please check the official https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.1/html-single/running_data_grid_on_openshift/index#installation[documentation].


=== Deploying a RHDG cluster

Data Grid Operator lets you create, configure, and manage Data Grid clusters. Data Grid Operator adds a new Custom Resource (CR) of type Infinispan that lets you handle Data Grid clusters as complex units on OpenShift.

Data Grid Operator watches for Infinispan Custom Resources (CR) that you use to instantiate and configure Data Grid clusters and manage OpenShift resources, such as StatefulSets and Services. In this way, the Infinispan CR is your primary interface to Data Grid on OpenShift.


I have created an OCP template to quickly deploy a basic RHDG cluster with 3 replicas. Execute the following command have it up and running on your cluster. 


[source, bash]
----
oc process -f rhdg-operator/rhdg-02-cluster.yaml | oc apply -f -
----

This template provides two parameters to modify the project where the cluster is installed and the name of the cluster to deploy. The cluster namespace should be the same as in the previous step. By default, values are: 

* *CLUSTER_NAMESPACE* = `rhdg8`
* *CLUSTER_NAME* = `rhdg`


Modify them just passing arguments to the template:

[source, bash]
----
oc process -f rhdg-operator/rhdg-02-cluster.yaml -p CLUSTER_NAMESPACE="another-namespace" -p CLUSTER_NAME="my-cluster" | oc apply -f -
----


=== Creating RHDG caches using the Operator CRD

WARNING: Creating caches with Data Grid Operator is available as a technology preview. If you want to create caches in a production environment, I encourage you to use the REST or Java alternatives.


Data Grid stores entries into caches, which can be created using several methods: REST, CLI, programmatically using the Java Client or using the `Cache` CRD. In this section, we will explore how to create caches using the Operator.


NOTE: For other ways of creating caches, please check this other https://github.com/alvarolop/rhdg8-client#cache-configuration[Git repository] with information about the Data Grid client.

To create caches with Data Grid Operator, you use Cache CRs to add caches from templates or XML configuration. Bear in mind the following contrains:

* You can create a single cache for each `Cache` CR.
* If you edit caches in the OpenShift Web Console, changes do not take effect on the Data Grid cluster. You must delete the CR and create it again with the new configuration.
* Deleting Cache CRs in the OpenShift Web Console, does not remove caches from Data Grid clusters. You must delete caches through the console or CLI.


I have created an OCP template to quickly set up two caches on the RHDG cluster:

* `operator-cache-01`: Based on an xml configuration.
* `operator-cache-02`: Based on an already defined templated.


In order to apply this template, just execute the following command:
[source, bash]
----
oc process -f rhdg-operator/rhdg-03-caches.yaml | oc apply -f -
----

This template provides two parameters to modify the project where the cluster is installed and the name of the cluster to deploy. The cluster namespace should be the same as in the previous step. By default, values are: 

* *CLUSTER_NAMESPACE* = `rhdg8`
* *CLUSTER_NAME* = `rhdg`


Modify them just passing arguments to the template:

[source, bash]
----
oc process -f rhdg-operator/rhdg-03-caches.yaml -p CLUSTER_NAMESPACE="another-namespace" -p CLUSTER_NAME="my-cluster" | oc apply -f -
----

Interact with the newly created caches with he following commands:
[source, bash]
----
# Set your variables. These are default:
CLUSTER_NAMESPACE="rhdg8"
CLUSTER_NAME="rhdg"
RHDG_URL=$(oc get route ${CLUSTER_NAME}-external -n ${CLUSTER_NAMESPACE} -o template='https://{{.spec.host}}')

# Check all the caches on your cluster
curl -X GET -k -u developer:developer -H "Content-Type: application/json" ${RHDG_URL}/rest/v2/caches | jq

# Check information about an specific cache
curl -X GET -k -u developer:developer -H "Content-Type: application/json" ${RHDG_URL}/rest/v2/caches/${CACHE_NAME} | jq

# Delete a cache
curl -X DELETE -k -u developer:developer ${RHDG_URL}/rest/v2/caches/${CACHE_NAME}
----


For more information about how to create caches using the CRD, please check the https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.1/html-single/running_data_grid_on_openshift/index#creating_caches_operator-caches[official documentation].


=== Monitoring RHDG with Prometheus

Data Grid exposes a metrics endpoint that provides statistics and events to Prometheus.

After installing OpenShift Container Platform 4.6, cluster administrators can optionally enable monitoring for user-defined projects. By using this feature, cluster administrators, developers, and other users can specify how services and pods are monitored in their own projects. You can then query metrics, review dashboards, and manage alerting rules and silences for your own projects in the OpenShift Container Platform web console. We are going to take advantage of this feature.


.Enabling monitoring for user-defined projects
[WARNING]
==== 
Monitoring of user-defined projects is not enabled by default. To enable it, you need to modify a Configmap of the `openshift-monitoring`. You need permissions to create and modify Configmaps in this project. You only have to execute this command *once* per namespace. [underline]#Please, do not execute it before checking if this was done before, you can override work from your colleagues#

[source, bash]
----
oc apply -f ocp/ocp-01-user-workload-monitoring.yaml
----

After executing the command above, you will see some pods in the following namespace:
[source, bash]
----
oc get pods -n openshift-user-workload-monitoring
----
====

// In order to access the Prometheus that will contain the metrics of DG, expose its service:
// [source, bash]
// ----
// oc expose svc/prometheus-user-workload -n openshift-user-workload-monitoring
// ----


I have created an OCP template to quickly configure metrics monitorization of a RHDG cluster. Execute the following command:

[source, bash]
----
oc process -f rhdg-operator/rhdg-04-monitoring.yaml | oc apply -f -
----

This template provides two parameters to modify the project where the cluster was installed and the name of the cluster itself. By default, values are: 

* *CLUSTER_NAMESPACE* = `rhdg8`
* *CLUSTER_NAME* = `rhdg`


Modify them just passing arguments to the template:

[source, bash]
----
oc process -f rhdg-operator/rhdg-04-monitoring.yaml -p CLUSTER_NAMESPACE="another-namespace" -p CLUSTER_NAME="my-cluster" | oc apply -f -
----

For more information, access the Openshift https://docs.openshift.com/container-platform/4.6/monitoring/understanding-the-monitoring-stack.html[documentation] for the monitoring stack and the RHDG documenation to https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.1/html-single/running_data_grid_on_openshift/index#prometheus[configure monitoring] for RHDG 8 on OCP.



== Monitoring RHDG with Grafana

A typical OpenShift monitoring stack includes Prometheus for monitoring both systems and services, and Grafana for analyzing and visualizing metrics.

Administrators are often looking to write custom queries and create custom dashboards in Grafana. However, Grafana instances provided with the monitoring stack (and its dashboards) are read-only.  To solve this problem, we can use the community-powered Grafana operator provided by OperatorHub.

To deploy the community-powered Grafana operator on OCP 4.6 just follow these steps:

=== Deploy the Grafana operator
[source, bash]
----
oc process -f grafana/grafana-01-operator.yaml | oc apply -f -
----

=== Create a Grafana instance
Now, we will create a Grafana instance using the operator:
[source, bash]
----
oc process -f grafana/grafana-02-instance.yaml | oc apply -f -
----

=== Create a Grafana data source
Now, we will create a Grafana data source:
[source, bash]
----
PROJECT=grafana

oc adm policy add-cluster-role-to-user cluster-monitoring-view -z grafana-serviceaccount -n ${PROJECT}
BEARER_TOKEN=$(oc serviceaccounts get-token grafana-serviceaccount -n ${PROJECT})
oc process -f grafana/grafana-03-datasource.yaml -p BEARER_TOKEN=${BEARER_TOKEN} | oc apply -f -
----

=== Create a Grafana dashboard
Now, we will create a Grafana dashboard:
[source, bash]
----
DASHBOARD_NAME="grafana-dashboard-rhdg8"
# Create a configMap containing the Dashboard
oc create configmap $DASHBOARD_NAME --from-file=dashboard=grafana/$DASHBOARD_NAME.json -n $PROJECT
# Create a Dashboard object that automatically updates Grafana
oc process -f grafana/grafana-04-dashboard.yaml -p DASHBOARD_NAME=$DASHBOARD_NAME | oc apply -f -
----

NOTE: https://github.com/grafana-operator/grafana-operator/blob/master/deploy/examples/dashboards[Here] you can find information of other ways of creating dashboards.


=== Access Grafana as admin

After accessing Grafana using the OCP SSO, you may log in as admin. Retrieve the credentials from the secret using the following commands:
[source, bash]
----
oc get secret grafana-admin-credentials -n $PROJECT -o jsonpath='{.data.GF_SECURITY_ADMIN_USER}' | base64 --decode
oc get secret grafana-admin-credentials -n $PROJECT -o jsonpath='{.data.GF_SECURITY_ADMIN_PASSWORD}' | base64 --decode
----



For more information, access the Grafana https://grafana.com/docs/grafana/latest/[main documentation] or the Grafana https://github.com/integr8ly/grafana-operator/blob/v3.6.0/README.md[operator documentation].




== Alerting with PrometheusRules


[source, bash]
----
oc process -f rhdg-operator/rhdg-05-alerting-rules.yaml | oc apply -f -
----




:sectnums!:


== Annex: Configure the scope of your operator

An Operator group, defined by the OperatorGroup resource, provides multitenant configuration to OLM-installed Operators. An Operator group selects target namespaces in which to generate required RBAC access for its member Operators.

If you want to modify the default behavior of the template provided in this repository, modify lines 26 to 33 of this link:rhdg/rhdg-01-operator.yaml[template].

1) *AllNamespaces*: The Operator can be a member of an Operator group that selects all namespaces (target namespace set is the empty string ""). This configuration allows us to create DG clusters in every namespace of the cluster:

[source, yaml]
----
- apiVersion: operators.coreos.com/v1
  kind: OperatorGroup
  metadata:
    name: datagrid
    namespace: ${OPERATOR_NAMESPACE}
  spec: {}
----

2) *MultiNamespace*: The Operator can be a member of an Operator group that selects more than one namespace. Choose this option if you want to have several operators that manage RHDG clusters. For example, if you want to have a different operator per Business Unit managing several Openshift projects:
[source, yaml]
----
- apiVersion: operators.coreos.com/v1
  kind: OperatorGroup
  metadata:
    name: datagrid
    namespace: ${OPERATOR_NAMESPACE}
  spec:
    targetNamespaces:
      - ${CLUSTER_NAMESPACE-1}
      - ${CLUSTER_NAMESPACE-2}
----

3) *SingleNamespace*: The Operator can be a member of an Operator group that selects one namespace. This is useful if we want every application (Each OCP namespace) to be able to configure and deploy their own DG clusters:

[source, yaml]
----
- apiVersion: operators.coreos.com/v1
  kind: OperatorGroup
  metadata:
    name: datagrid
    namespace: ${OPERATOR_NAMESPACE}
  spec:
    targetNamespaces:
      - ${CLUSTER_NAMESPACE}
----

For more information, check the Openshift https://docs.openshift.com/container-platform/4.6/operators/understanding/olm/olm-understanding-operatorgroups.html[documentation] about Operator Groups and the official https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.1/html-single/running_data_grid_on_openshift/index#install_automated-install[documentation] to install DG on Openshift.



== Annex: Stern - Tail logs from multiple pods

In some situations, you will need to monitor logs from several pods of the same application and maybe you want to check to which pod did the request arrived. https://github.com/wercker/stern[Stern] allows you to tail multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging.

First, you will need to install it on your machine. After that, log in to your cluster and monitoring the previous deployment is as simple as executing the following command:

[source, bash]
----
stern --namespace=$CLUSTER_NAMESPACE -l clusterName=$CLUSTER_NAME
----

The previous command will show all the logs from all the pods from a namespace that contain a given label. 

There are many filters and configuration options. Check the https://github.com/wercker/stern#cli-flags[documentation] for a full list of them




== Annex: Deploying the Infinispan operator

The same configuration rules from the previous chapter apply.

[source, bash, linenums]
----
oc process -f rhdg-operator/infinispan-01-operator.yaml -p OPERATOR_NAMESPACE="infinispan-operator" -p CLUSTER_NAMESPACE="infinispan" | oc apply -f -
----

It is also possible to install the operator from the web console. For more information, please check the official https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.1/html-single/running_data_grid_on_openshift/index#installation[documentation].






== Annex: Deploying RHDG 8 without operator

WARNING: Bear in mind that this template is for testing purposes and that the installation made with it will not benefit from any kind of Red Hat support.

If you need to test any configuration that the operator does not provide, it is possible to deploy the RHDG cluster manually creating all the Openshift resources that the Operator manages automagically.

By default, the template deploys a RHDG cluster named `rhdg` in the project `rhdg8-operatorless`. Deploy your cluster performing the following command:

[source, bash]
----
oc process -f rhdg-template/rhdg-99-operatorless.yaml | oc apply -f -
----


Due to the limitation of OCP templates, if you want to change the cluster name and the namespace, you will have to edit lines #103, and #107 of the file `rhdg-template/rhdg-99-operatorless.yaml`. This is the pattern that you have to follow [Substitute the ENV_VARS with the correct values]:

[source, yaml]
----
  data:
    infinispan.yaml: |
      clusterName: ${CLUSTER_NAME}
      jgroups:
        transport: tcp
        dnsPing:
          query: ${CLUSTER_NAME}-ping.${CLUSTER_NAMESPACE}.svc.cluster.local
        diagnostics: false
----

After modifying the yaml, process the template with the correct params:

[source, bash]
----
oc process -f rhdg-template/rhdg-99-operatorless.yaml -p CLUSTER_NAMESPACE="another-namespace" -p CLUSTER_NAME="my-cluster" | oc apply -f -
----


== Annex: Deploy RHDG with XSite configuration


WARNING: This configuration only works for two different OCP clusters and templates need extra configuration. Do not try to configure it on the same OCP cluster as this will only be available in DG 8.2 onwards.

Create the namespaces and deploy the operator: 
[source, bash]
----
oc process -f rhdg-operator/rhdg-01-operator-xsite.yaml \
  -p OPERATOR_NAMESPACE="rhdg8-xsite-operator" \
  -p CLUSTER_NAMESPACE_XSITE_A="rhdg8-xsite-01" \
  -p CLUSTER_NAMESPACE_XSITE_B="rhdg8-xsite-02" | oc apply -f -
----


Allow service accounts to access the API to check the DG clusters:
[source, bash]
----
# Option 1: Documentation
oc policy add-role-to-user view system:serviceaccount:rhdg8-xsite-01:rhdg-xsite-01
oc policy add-role-to-user view system:serviceaccount:rhdg8-xsite-02:rhdg-xsite-02
# Option 2: DG Teams group
oc -n rhdg8-xsite-01 adm policy add-cluster-role-to-user cluster-reader -z rhdg-xsite-01
oc -n rhdg8-xsite-02 adm policy add-cluster-role-to-user cluster-reader -z rhdg-xsite-02
----


Create cluster in OCP cluster 1:
[source, bash]
----
oc process -f rhdg-operator/rhdg-02-cluster-xsite.yaml \
  -p CLUSTER_NAMESPACE="rhdg8-xsite-01" \
  -p CLUSTER_NAME="rhdg-xsite-01" \
  -p CLUSTER_NAMESPACE_XSITE_REMOTE="rhdg8-xsite-02" \
  -p CLUSTER_NAME_XSITE_REMOTE="rhdg-xsite-02" | oc apply -f -
----

Create cluster in OCP cluster 2:
[source, bash]
----
oc process -f rhdg-operator/rhdg-02-cluster-xsite.yaml \
  -p CLUSTER_NAMESPACE="rhdg8-xsite-02" \
  -p CLUSTER_NAME="rhdg-xsite-02" \
  -p CLUSTER_NAMESPACE_XSITE_REMOTE="rhdg8-xsite-01" \
  -p CLUSTER_NAME_XSITE_REMOTE="rhdg-xsite-01" | oc apply -f -
----

Share tokens of the SA from one cluster to the other:
[source, bash]
----
TOKEN_XSITE_01=$(oc sa get-token rhdg-xsite-01 -n rhdg8-xsite-01)
TOKEN_XSITE_02=$(oc sa get-token rhdg-xsite-02 -n rhdg8-xsite-02)

oc create secret generic rhdg-xsite-01-xsite-secret -n rhdg8-xsite-01 --from-literal=token=${TOKEN_XSITE_01}
oc create secret generic rhdg-xsite-01-xsite-secret -n rhdg8-xsite-02 --from-literal=token=${TOKEN_XSITE_01}

oc create secret generic rhdg-xsite-02-xsite-secret -n rhdg8-xsite-01 --from-literal=token=${TOKEN_XSITE_02}
oc create secret generic rhdg-xsite-02-xsite-secret -n rhdg8-xsite-02 --from-literal=token=${TOKEN_XSITE_02}
----



== Annex: Retrieve server reports from Openshift


[source, bash]
----
oc project $RHDG_NAMESPACE
for pod in $(oc get pods -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}')
do
  echo "$pod: Generate report"
  oc exec $pod -- bash -c 'echo "server report" | ./bin/cli.sh -c $HOSTNAME:$RHDG_SERVICE_PORT_INFINISPAN -f -'
  echo "$pod: Download report"
  oc exec $pod -- bash -c 'files=( *tar.gz* ); cat "${files[0]}"' > $(date +"%Y-%m-%d-%H-%M")-$pod-report.tar.gz
  echo "$pod: Remove report"
  oc exec rhdg-0 -- bash -c 'rm -rf *tar.gz*'
done
----


== Annex: Retrieve Queries stats

[source, bash]
----
oc project $RHDG_NAMESPACE
for pod in $(oc get pods -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}')
do
  echo "$pod: Get stats"
  oc exec $pod -- bash -c 'curl $HOSTNAME:$RHDG_SERVICE_PORT_INFINISPAN/rest/v2/$CACHE_NAME/operator-cache-01/search/stats' | jq
done
----







== Useful links

* https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.1/[RHDG 8.1 documentation].
* https://infinispan.org/documentation[Upstream documentation].
* https://access.redhat.com/articles/4933551[RHDG 8 Supported Configurations].
* https://access.redhat.com/articles/4933371[RHDG 8 Component Details].
* https://access.redhat.com/articles/4961121[RHDG 8 Maintenance Schedule].
* https://access.redhat.com/support/policy/updates/jboss_notes/#p_rhdg[RHDG Product Update and Support Policy].
* https://developers.redhat.com/blog/2020/10/15/securely-connect-quarkus-and-red-hat-data-grid-on-red-hat-openshift[Securely connect Quarkus and RHDG 8.1 on OCP].